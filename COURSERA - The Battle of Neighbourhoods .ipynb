{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Coursera Capstone - Battle of the Neighbourhoods (Week 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As part of the final module for the IBM Data Science Professional Certificate, I am tasked with using all skills, technologies and methods learnt throughout the 9 module course, alongside utilising the foursquare API and any relevant accessible data, to experiment with real world problems in the data science world. \n\nFor this week, I will be providing:\n\n- A description of the problem and a discussion of the background. \n- A description of the data and how it will be used to solve the problem."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1. Discussion and Background of the Business Problem"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "BrightMindsDS is a new tech company that specialises in Machine Learning and Big Data. The start up originally began in the CEO's garage in Kent, with 4 employees trying to do everything. Since landing 6 new clients they have had just cause to expand the business and move their office to a more professional location in London, England. The founder of BrightMindsDS requires at least 10 more employees and wants to find a suitable office space to encourage growth, innovation and creativity. \n\n\"Companies that care more about health and welfare of their employees enjoy better engagement, while employees are less often sick, meaning productivity receives a boost. With this in mind, a new report has encouraged employers to think more about how they can make their employees more healthy.\" (1) \n\n\"Although success can bring happiness, to create great work, the employees\u2019 health, wellbeing and happiness is the most important factor.\" (2)\n\nWith this in mind, the CEO wants to create a nice atmosphere for his new employees. He believes that his employees' health and wellbeing will be a main driver towards the company's success. When looking at possible locations for the new office, things like:\n\n- Can it be accessed via local transport? \n- Are there any gyms nearby?\n- Is there a nice park/open space near by?\n- Are there good coffee shops nearby for lunch?\n- Are there good bars/nightclubs to socialise in after work?\n\n\nwill be considered. A minimum of 2 of these requirements is ideal for a new office location.\n\nIn order to obtain an ideal location for the new office space, the London area is explored through clustering and segmentation based on London Post code data and fulfilment of the requirements.\n\n(1) https://www.consultancy.uk/news/18201/companies-that-invest-in-employee-health-enjoy-higher-engagement\n\n(2) https://inside.6q.io/employee-health-wellbeing-matters/"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Target Audience \n\nAlthough this project looks at a new Tech company, a variety of companies would be interested in the methods used in this project:\n\n- Any start up or small company might find this useful when looking to expand.\n- Any large company that is hoping to open a new office in an unknown location may use this to compare different places. \n- This project is so adaptable that many types of venues, e.g. cafes, restaurants, museums, nightclubs, etc, can use the methods from this project to open in a different location. \n\nThis project's main audience is anyone who is looking to open a new space in a different location to where they are currently situated. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Data Preparation "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Overview\nThe data for this project will be collected from the Foursquare API and Wikipedia. From wikipedia I can obtain a list of all of the boroughs in London, alongside their postal codes. Using Foursquare I can obtain relevant information about the selected areas.\n\nLondon synonymous to the Greater London Area, is the capital and largest city of England and of the United Kingdom. It encompasses a total area of 1,583 square kilometres (611 sq mi), an area which had a population of 7,172,036 in 2001 and a population density of 4,542 inhabitants per square kilometre (11,760/sq mi). There are 32 Boroughs in Greater London as well as the 'City of London' principal division. There are 8 Post Code areas; N, NW, SW, SE, W, WC, E and EC. And 121 post code districts. As to be expected this is a very large data set. \n\nData can be found here: <https://en.wikipedia.org/wiki/List_of_areas_of_London >\n\nBasic research has led me to focus on East London, i.e. Boroughs with the Post Code starting with E or EC. I have made this decision as this is where the 'East London Tech City' can be found. It is said to be a great location to build your company.\n\n\"Some ask why they should set up there. I say the cluster is where your business is most likely to succeed. Your partners clients, investors will be there. You\u2019ll be able to hire people. You\u2019ll come up with great ideas.\" (3)\n\nUsing python libraries and frameworks I will be able to scrape these wiki pages to obtain this relevant data;\n\n- Location\n- Borough  \n- Post-town\n- Post Code\n- Dial Code\n- Grid reference \n\nThis data will be filtered to contain only East London locations. \n\nUsing the Geocoder package I can get the Latitude and Longitude for each of these locations. \n\nThe Foursquare API allows application developers to interact with the Foursquare platform. The API itself is a RESTful set of addresses to which you can send requests, so there's really nothing to download onto your server. \nYou can currently request output in XML or JSON format. \n\nUsing the Foursquare API I will be able to make specific requests about set locations. I hope to collect a vast amount of data to help fullfil BrightMindsDS requirements and answer their questions. \n\nDepending on how much data is available will also help determine whether a location is suitable. Again, this will be relevant to the questions; e.g.\n\nThe client will want at least 5 coffee shops nearby with a minimum 4 star rating but will only require 1 park/open space to be close to the office. If there are only 2 gyms that shoudn't be a problem compared to if there were only 2 bars in the local area etc. \n\nData I hope to obtain:\n\n- Type of venue\n- Distance from neighbourhood \n- Latitude\n- Longitude \n\nLooking at the Foursquare API data, I will hopefully be provided with relevant, personal and reliable data. Using these factors will help to decide where the best location will be to set up the new BrightMindsDS office. \n\n\n\n(3) https://www.mbymontcalm.co.uk/blog/all-you-need-to-know-about-east-london-tech-city/\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Cleaning and Feature Selection \n\nThroughout this notebook, the data will be analysed, broken down and cleaned. Each step I take will be clearly documented and the thought process behind it explained. \n\n\nFor readability I will download all libraries here first.\n\n!! INCLUDE MORE INFORMATION ABOUT THESE LIBRARIES IN THE REPORT\n\n- beautiful soup\n- numpy and pandas, matplotlib\n- geopy and geocoder \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# library for BeautifulSoup, for web scrapping\nfrom bs4 import BeautifulSoup\n\n# library to handle data in a vectorized manner\nimport numpy as np\n\n# library for data analsysis\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# library to handle JSON files\nimport json\nprint('numpy, pandas, ..., imported...')\n\n!pip -q install geopy\nprint('geopy installed...')\n\n# convert an address into latitude and longitude values\nfrom geopy.geocoders import Nominatim\nprint('Nominatim imported...')\n\n# library to handle requests\nimport requests\nprint('requests imported...')\n\n# tranform JSON file into a pandas dataframe\nfrom pandas.io.json import json_normalize\nprint('json_normalize imported...')\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nprint('matplotlib imported...')\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\nprint('Kmeans imported...')\n\n# install the Geocoder\n!pip -q install geocoder\nimport geocoder\n\n# import time\nimport time\n\n# install and import folium \n!pip -q install folium\nprint('folium installed...')\nimport folium # map rendering library\nprint('folium imported...')\n\n# install and import mpu for distance calculations using lat and long\n!pip install mpu --user\nimport mpu\n\nprint('...Done')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Use BeautifulSoup library to scrape the wikipedia page for the post codes, neighbourhoods and boroughs. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# specify which URL/web page we are going to be scraping\nwebsite_url = requests.get(\"https://en.wikipedia.org/wiki/List_of_areas_of_London\").text\n\n# parse the HTML from our URL into the BeautifulSoup parse tree format\nsoup = BeautifulSoup(website_url, 'lxml')\n\n# use the BeautifulSoup function 'prettify' to look a the HTML underlying our chosen web page \nprint(soup.prettify())"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use the 'find' function to bring back the table data only using the class id: 'wikitable sortable' (found in code print out above)\n\nMy_table = soup.find('table', {'class':'wikitable sortable'})\nMy_table"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Loop through data \nA = []\nB = []\nC = []\nD = []\nE = []\nF = []\n\nfor row in My_table.findAll('tr'):\n    cells=row.findAll('td')\n    if len(cells)==6:\n        A.append(cells[0].find(text=True))\n        B.append(cells[1].find(text=True))\n        C.append(cells[2].find(text=True))\n        D.append(cells[3].find(text=True))\n        E.append(cells[4].find(text=True))\n        F.append(cells[5].find(text=True))\n\nLondon_data = pd.DataFrame(A, columns=['Neighbourhood'])\nLondon_data['Borough'] = B\nLondon_data['Post_town'] = C\nLondon_data['Postcode'] = D\nLondon_data['Dial_code'] = E\nLondon_data['OS_grid_ref'] = F\n\n\nLondon_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As can be seen, some locations are associated to two postcodes, e.g. Acton; W3 and W4. The data is **cleaned** by spreading the postcodes to multi-rows and assigning the same values from the other columns. Aside from this, the data collected contained no NULL values or duplicated rows. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# split locations with more than 1 postcode into multiple rows\nclean_df = London_data.drop('Postcode', axis=1).join(London_data['Postcode'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('Postcode'))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "clean_df.head(10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "clean_df.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### FEATURE SELECTION prt1 - only select columns required\n\nAfter data cleaning I was left with 6 columns and 636 rows of data.\n\nFeature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n\nFeature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\nHaving irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n\nBenefits of Feature Selection:\n\n\u00b7 **Reduces Overfitting**: Less redundant data means less opportunity to make decisions based on noise.\n\n\u00b7 **Improves Accuracy**: Less misleading data means modeling accuracy improves.\n\n\u00b7 **Reduces Training Time**: fewer data points reduce algorithm complexity and algorithms train faster.\n\nLooking at the data collected from wikipedia, it was clear that not all of it was needed. Phone codes would have no impact on the comapany's choice of location so the *Dial_code* column could be removed. Grid references although helpful when using a map, are not required to obtain the longitude and latitude of the neighbourhoods. That can be done with the relevant libraries and Postcodes. This lead to the *'OS_grid_ref'* column being removed also. \n\nThis left the columns: *Neighbourhood*, *Borough*, *Post_town* and *Postcode*."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# I removed the Dial_code and OS_grid_ref columns as neither were providing any additional information. \nclean_df = clean_df[['Neighbourhood', 'Borough', 'Postcode', 'Post_town']].reset_index(drop=True)\nclean_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Obtain Coordinates of each location and use folium to visualise the data \n\nTo be able to visualise this data, I need the latitude and longitude of each of the locations in my clean London data frame. To do this I will use the Geocoder package. The Geocoder package is used with the *arcgis_geocoder*."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Geocoder starts here\n# Defining a function to use --> get_latlng()'''\ndef get_latlng(arcgis_geocoder):\n    \n    # Initialize the Location (lat. and long.) to \"None\"\n    lat_lng_coords = None\n    \n    # While loop helps to create a continous run until all the location coordinates are geocoded\n    while(lat_lng_coords is None):\n        g = geocoder.arcgis('{}, London, United Kingdom'.format(arcgis_geocoder))\n        lat_lng_coords = g.latlng\n    return lat_lng_coords\n# Geocoder ends here"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Test the function with a random postcode \nsample = get_latlng('EC3')\nsample "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To make sure this has worked correctly, we can reverse geocode this, using the geocodefarm geocoder. It gives the following:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Reverse using to geocodefarm to test my function\ngg = geocoder.geocodefarm(sample, method = 'reverse')\ngg"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As can be seen, this function has correctly identified the coordinate for that postcode. This means it can be used on my *clean_df* dataframe. For testing purposes or to help clients with large amounts of data, I have added a time parameter to determine how long the function will take to run on my dataset. \n\n*N.B. Due to the size of this data frame, this will take a while.(around 5mins)* "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "start = time.time()\n\n# Extract postcodes \npostal_codes = clean_df['Postcode'] \n\n# Use latlng function to obtain coordinates and add them to a list called coordinates\ncoordinates = [get_latlng(postal_code) for postal_code in postal_codes.tolist()]\n\nend = time.time()\nprint(\"Time of execution: \", end - start, \"seconds\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The obtained coordinates (latitude and longitude) are joined with the dataframe as shown\nlondon_coordinates = pd.DataFrame(coordinates, columns = ['Latitude', 'Longitude'])\nclean_df['Latitude'] = london_coordinates['Latitude']\nclean_df['Longitude'] = london_coordinates['Longitude']\nclean_df.head(20)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Using this data alongside the folium visualisation library,  I can create a map of London with the neighbourhoods superimposed on top. \nThis is a helpful visual to show how the data is spread out over the City and see if any observations can be made from it. Clicking on the blue markers reveals the name of each neighbourhood, the respective borough and the Postcode.\n\nI also need the latitude and longitude of London. This is technically the 'starting point' for the map. I can obtain this by using geocoder. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# To define an instance of the geocoder, we need to define a user_agent\naddress = 'London, England'\n\n# Use this to obtain the longitude and latitude of London \ngeolocator = Nominatim(user_agent=\"to_explorer\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinates of London are {}, {}.'.format(latitude, longitude))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Create a map of Toronto using latitude and longitude values\nmap_london = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, neighbourhood, borough, postcode in zip(clean_df['Latitude'], clean_df['Longitude'], clean_df['Neighbourhood'], clean_df['Borough'], clean_df['Postcode']):\n    label = '{}, {}, {}'.format(neighbourhood, borough, postcode)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_london)  \n    \nmap_london"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This map provides a nice overview of the data. It is interactive which can help determine where neighbourhoods are in relation to each other. However, due to the limitations of the free Foursquare option, I need to decrease the size of my df meaning further **cleaning** is required. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use .shape to ascertain how big the curret df is\nclean_df.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Count of how many neighbourhoods per postcode \npost_count = clean_df['Postcode'].value_counts()\nprint(post_count)\n\nprint('------------------------------')\n# Count of how many unique postcodes there are\nprint(post_count.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "There are 288 unique postcodes and 636 rows of data. This is a great data set but too large for this project. The foursquare API collects data relevant to all neighbourhoods in the dataframe. Obtaining venue information with reagrds to each neighbourhood will be too much information to work with. To reduce the size appropriately without removing any relevant data, I am going to focus on the **East London** postcodes only, as discused above. To do this, the data set needs to be cleaned and filtered again. I have decided to do this by:\n\n- Only keeping data where the Post_town = LONDON \n- Only taking data where the Postcode contains 'E'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Only select rows where post town is LONDON\nlondon_towns = clean_df[clean_df['Post_town'].str.contains('LONDON')]\nlondon_towns.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Only select rows where the Location is in East London - Postcode contains an 'E'\n\n# re-assigns to east_london\nEL = london_towns \n\n# Strips whitespaces before postcode\nEL.Postcode = EL.Postcode.str.strip()\n\n# New dataframe for East London postcodes \neast_london = EL[EL['Postcode'].str.contains(('E'))].reset_index(drop=True)\n\neast_london.head(10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Determine the size of the new dataframe \neast_london.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "156 rows is a much smaller data set to be using, hopefully making visuals clearer and the data easier to understand. All rows now only look at neighbourhoods situated in the East of London. Recreating the map with this new data set helps add context to where I am considering for a new office space. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Create a map of East London using latitude and longitude values\nmap_east_london = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, neighbourhood, borough, postcode in zip(east_london['Latitude'], east_london['Longitude'], east_london['Neighbourhood'], east_london['Borough'], east_london['Postcode']):\n    label = '{}, {}, {}'.format(neighbourhood, borough, postcode)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_east_london)  \n    \nmap_east_london"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Foursquare \n\nNow we look at using the Foursquare API. \n\nFoursquare is a social location service that allows users to explore the world around them. \nThe Foursquare API allows application developers to interact with the Foursquare platform. The API itself is a RESTful set of addresses to which you can send requests, so there's really nothing to download onto your server. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "CLIENT_ID = 'ADGKVF32C0BQ4WCU1VBVEGQHWNIYRI2NPR0EOP4LKBKP4KCZ' # your Foursquare ID\nCLIENT_SECRET = 'PK14IB5RQETEYWSGU2F5OUFF3XCS1BY2ZZ0ZFMA1OEHO3UNQ' # your Foursquare Secret\nVERSION = '20180605' # Foursquare API version\n\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# obtain the nearest 150 venues in each neighbourhood\ndef getNearbyVenues(names, latitudes, longitudes, radius=2000):\n    \n    LIMIT = 150\n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighbourhood', \n                  'Neighbourhood Latitude', \n                  'Neighbourhood Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use the nearby venues function to get all venues for all neighbourhoods within my east_london dataframe\nel_venues = getNearbyVenues(names=east_london['Neighbourhood'],\n                                   latitudes=east_london['Latitude'],\n                                   longitudes=east_london['Longitude']\n                                  )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "# Have a brief look at the data collected from Foursquare\nel_venues.head(20)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Methodology\n\nHere I can start to play around with the data and have a look at what information Foursquare has made available to me. \n\nI started by breaking the data down by neighbourhoods; looking at how many venues are in each location. This shows which areas of london may have more going on compared to quieter settings with less to do. \n\nLooking back at the requirements set out by BrightMindsDS I can assess whether they are realistic, e.g. if across all neighbourhoods there are only 2 parks, this requirement may be difficult to fulfil. Fortunately, all venues fitting the criteria listed by the company had a high total count across East London. Creating a bar chat, this made it easier to visualise. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get a count of how many venues there are per neighbourhood\nel_venues_count = el_venues.groupby('Neighbourhood').count()\nel_venues_count"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get a count of all the unique venue categories in this dataset \nprint('There are {} uniques categories.'.format(len(el_venues['Venue Category'].unique())))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get a count of each venue category across all neighbourhoods \nel_venues_unique_count = el_venues['Venue Category'].value_counts().to_frame(name='Count')\nel_venues_unique_count = pd.DataFrame(el_venues_unique_count).reset_index()\nel_venues_unique_count.columns = ['Venue', 'count']\nel_venues_unique_count"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Graph to show all venue categories that have more than 50 sites in East London\nfilt_data = pd.DataFrame(el_venues_unique_count[el_venues_unique_count['count']>50])\nlabel = filt_data['Venue']\ndata = filt_data['count']\n\n# Using the matplotlib.pyplot library to generate a bar chart \n\ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(label))\n    plt.figure(figsize = (20,10))\n    plt.bar(index, data)\n    plt.xlabel('Venue Type', fontsize=10)\n    plt.ylabel('Total number of venues', fontsize=10)\n    plt.xticks(index, label, fontsize=10, rotation = 90)\n    plt.title('Count of venues across the whole of East London')\n    plt.show()\n    \n\n    \nplot_bar_x()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "These methods offer a lovely breakdown of the collected Foursquare data. There is a wide variety of venues with a great mix of categories. \nFrom this I can see all varieties of venues that are in BrightMindsDS' criteria have at least 50 individual sites in East London. This means I have a decent amount of data to help determine the best locations available. \n\nTo make sure I have only relevant data to cluster, I am going to section my dataset based on the requirements set by the company:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Obtain neighbourhoods that have some type of public transport station in them \nEL_stations = el_venues[el_venues['Venue Category']\\\n                       .str.contains(' Station')].reset_index(drop=True)\nEL_stations.index = np.arange(1, len(EL_stations)+1)\nprint(\"shape of df with venue category containing stations: \", EL_stations.shape)\nEL_stations.head(20)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Obtain neighbourhoods that have some type of coffee shop or cafe nearby \nEL_cafes = el_venues[el_venues['Venue Category']\\\n                       .str.contains('Coffee')].reset_index(drop=True)\nEL_cafes.index = np.arange(1, len(EL_cafes)+1)\nprint(\"shape of df with venue category is a coffee shop: \", EL_cafes.shape)\nEL_cafes.head(20)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Obtain neighbourhoods that have some type of bar or nightclub in them \nEL_bars = el_venues[el_venues['Venue Category']\\\n                       .str.contains(' Bar')].reset_index(drop=True)\nEL_bars.index = np.arange(1, len(EL_bars)+1)\nprint(\"shape of df with venue category is a bar/nightclub: \", EL_bars.shape)\nEL_bars.head(20)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Obtain neighbourhoods that have some type of gym or fitness center in them \nEL_gyms = el_venues[el_venues['Venue Category']\\\n                       .str.contains('Gym')].reset_index(drop=True)\nEL_gyms.index = np.arange(1, len(EL_gyms)+1)\nprint(\"shape of df with venue category is a gym: \", EL_gyms.shape)\nEL_gyms.head(20)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Obtain neighbourhoods that have some type of park or open space locally \nEL_park = el_venues[el_venues['Venue Category']\\\n                       .str.contains('Park')].reset_index(drop=True)\nEL_park.index = np.arange(1, len(EL_park)+1)\nprint(\"shape of df where venue category is a park: \", EL_park.shape)\nEL_park.head(20)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This was another example of **data cleaning**. I realised all data related to venues like museums, stadiums or zoos etc wasn't relevant and could negatively influence the machine learning model. This way I could only look at neighbourhoods that met a least 1 of the requirements. Saving time and resources spent looking at locations that couldn't satisfy the company. \n\nI sorted the list alphabetically by neighbourhood. Although very simple, doing this helped obtain a clearer picture of what venues can be found in each neighbourhood. I broke this down futher by looking at a count of unique venue categories per Neighbourhood. I visualised this using stacked bar graph. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Append all filtered data frames and print out the size to see how much data is available. \n\nEL_filtered_data = EL_stations.append(EL_cafes)\nEL_filtered_data = EL_filtered_data.append(EL_bars)\nEL_filtered_data = EL_filtered_data.append(EL_gyms)\nEL_filtered_data = EL_filtered_data.append(EL_park)\n\nEL_filtered_data.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Use .sort_values to order the dataset by 'Neighbourhood' - this way I can have a clearer view of what is available in each location\nEL_filtered_data_sort = EL_filtered_data.sort_values(by='Neighbourhood')\nEL_filtered_data_sort.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get a count of each type of venue per Neighbourhood\nuni_neigh = EL_filtered_data_sort.groupby(['Neighbourhood','Venue Category']).count()\nuni_neigh.reset_index()\n\nfirst_set = EL_filtered_data_sort.head(1313)\nlast_set = EL_filtered_data_sort.tail(1326)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#test = EL_filtered_data_sort.groupby(['Neighbourhood','Venue Category']).size().unstack().plot(kind='bar', stacked=True, figsize = (60,20))\ntest1 = first_set.groupby(['Neighbourhood','Venue Category']).size().unstack().plot(kind='bar', stacked=True, figsize = (40,15), fontsize = 10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test2 = last_set.groupby(['Neighbourhood','Venue Category']).size().unstack().plot(kind='bar', stacked=True, figsize = (40,15), fontsize = 10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Stacked bar charts are a great way to visualise trends and outliers. In this occasion, you can see Gay Bars do not appear very often, Elephant and Castle seems to have the largest count. Using the coloured keys it can be a nice way to pick out important information quickly. Due to the wide variety of venues it can be used for multiple purposes too. \n\nUsing the **Haversine function** I was able to calculate the distance between each venue and the respective neighbourhood. \n\n*The haversine formula is an equation important in navigation, giving great-circle distances between two points on a sphere from their longitudes and latitudes. It is a special case of a more general formula in spherical trigonometry, the law of haversines, relating the sides and angles of spherical \"triangles\".* https://rosettacode.org/wiki/Haversine_formula\n\nI did this not for the model, but as a good point of reference. If stuck between a choice of locations or unsure if a nightclub would be better than a pub, using distance can make the decision easier for BrightMindsDS. The more information you can give to a client the better the relationship will be. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use the Haversine function to obtain the distance between each venue and the relevant neighbourhood.(The distance will be in KM)\n\n# Extract each column and set as a variable \nNlat = EL_filtered_data_sort['Neighbourhood Latitude']\nNlon = EL_filtered_data_sort['Neighbourhood Longitude']\nVlat = EL_filtered_data_sort['Venue Latitude']\nVlon = EL_filtered_data_sort['Venue Longitude']\n\n\ndef Haversine(lat1,lon1,lat2,lon2, **kwarg):\n    \"\"\"\n    This uses the \u2018haversine\u2019 formula to calculate the great-circle distance between two points \u2013 that is, \n    the shortest distance over the earth\u2019s surface \u2013 giving an \u2018as-the-crow-flies\u2019 distance between the points \n    (ignoring any hills they fly over, of course!).\n    Haversine\n    formula:    a = sin\u00b2(\u0394\u03c6/2) + cos \u03c61 \u22c5 cos \u03c62 \u22c5 sin\u00b2(\u0394\u03bb/2)\n    c = 2 \u22c5 atan2( \u221aa, \u221a(1\u2212a) )\n    d = R \u22c5 c\n    where   \u03c6 is latitude, \u03bb is longitude, R is earth\u2019s radius (mean radius = 6,371km);\n    note that angles need to be in radians to pass to trig functions!\n    \"\"\"\n    R = 6371.0088\n    \n    # Convert each float to a radian \n    lat1,lon1,lat2,lon2 = map(np.radians, [lat1,lon1,lat2,lon2])\n\n    # relevant equations to find distance \n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2) **2\n    c = 2 * np.arctan2(a**0.5, (1-a)**0.5)\n    d = R * c\n    return round(d,4)\n\ndist = Haversine(Nlat, Nlon, Vlat, Vlon)\ndist.shape\ndist_df = pd.DataFrame(dist)\ndist_df.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Merge the new dataframe with the sorted one to link distance calcs with the relative venues and neighbourhoods\nel_dist = pd.concat([EL_filtered_data_sort, dist_df], axis = 1, ignore_index = True)\nel_dist.columns = ['Neighbourhood', 'Neighbourhood_lat', 'Neighbourhood_long', 'Venue', 'Venue_lat', 'Venue_long', 'Venue_Category', 'Distance_km']\nel_dist.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### One-hot encoding \n\nMany machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. In general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves. https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n\nThis means that categorical data must be converted to a numerical form. Hence the need for **One-hot encoding**.\nOne-hot encoding assigns common variables their own vector and gives them a value of 1 or 0. The length of these vectors is equal to the number of classes or categories the model is expected to classify. As a result the more variables in a column will now correspond with how often the variable appears in the vector.\n\nUsing my data as an example, each row is considered to be a vector. It will loop through the data set and assign a 1 to that column if the venue is associated with the respective neighbourhood or a 0 if not. Now all data is numerical, it can be inserted into a Machine Learning model. It will provide further insights into what is on offer in each neighbourhood"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# one hot encoding\nEL_onehot = pd.get_dummies(el_dist[['Venue_Category']], prefix = \"\", prefix_sep = \"\")\n\n# add neighborhood column back to dataframe\nEL_onehot['Neighbourhood'] = el_dist['Neighbourhood']\n\n# move neighborhood column to the first column\nfixed_columns = [EL_onehot.columns[-1]] + list(EL_onehot.columns[:-1])\nEL_onehot = EL_onehot[fixed_columns]\n\nEL_onehot.head(10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# group data to get one row or 'vector' per neighbourhood\nel_grouped = EL_onehot.groupby('Neighbourhood').mean().reset_index()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get the top 5 venue categories per neighbourhood\nnum_top_venues = 5 \nfor hood in el_grouped['Neighbourhood']:\n    print(\"----\"+hood+\"----\")\n    temp = el_grouped[el_grouped['Neighbourhood'] == hood].T.reset_index()\n    temp.columns = ['venue', 'freq']\n    temp = temp.iloc[1:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending = False).reset_index(drop = True).head(num_top_venues))\n    print('\\n')\n    \n   # use freq as % - offers a nice comparison per venue  "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# function to get the top 5 venues per neighbourhood \ndef return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending = False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# run function - print out new data frame \nnum_top_venues = 5\nindicators = ['st', 'nd', 'rd']\n# create columns according to number of top venues\ncolumns = ['Neighbourhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n# create a new dataframe\nneighbourhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighbourhoods_venues_sorted['Neighbourhood'] = el_grouped['Neighbourhood']\nfor ind in np.arange(el_grouped.shape[0]):\n    neighbourhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(el_grouped.iloc[ind, :], num_top_venues)\nneighbourhoods_venues_sorted.head(5)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we can see what venues are most popular per neighbourhood. This data has been retrieved from Foursquare limiting bias for or against specifics venues or venue types. Inputing this data for the KMeans Clustering algorithm will help distinguish patterns and similarities between neighbourhoods, hopefully offering BrightMindsDS a range of suitable locations. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### KMeans Clustering \n\n#### What is it?\n\nK-Means Clustering is an *unsupervised machine learning algorithm*. In contrast to traditional supervised machine learning algorithms, K-Means attempts to classify data without having first been trained with labeled data. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the most relevant group.\n\nhttps://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\n\n#### How does it work?\n\n1. Select K (i.e. 2) random points as cluster centers called centroids\n2. Assign each data point to the closest cluster by calculating its distance with respect to each centroid\n3. Determine the new cluster center by computing the average of the assigned points\n4. Repeat steps 2 and 3 until none of the cluster assignments change\n\n#### Choosing the right number of clusters\n\nOften, the data you\u2019ll be working with will have multiple dimensions making it difficult to visualise. As a consequence, the optimum number of clusters is hard to determine. Fortunately, I can use a mathematical way to determine this.\nI can graph the relationship between the number of clusters and Within Cluster Sum of Squares (WCSS), then by selecting the number of clusters where the change in WCSS begins to level off (elbow method). I have produced this graphically and can be seen below:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Using WCSS and the elbow method to determine the best 'k'\ndistortions = []\ndf = EL_onehot.iloc[:,1:-1]\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df)\n    distortions.append(kmeanModel.inertia_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Plot the graph to show at what value of k the distortion levels off\nplt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "looking at these results - elbow is at 5\nmeaning 5 clusters will be used"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "el_grouped_clustering = el_grouped.drop('Neighbourhood', 1)\nel_grouped_clustering.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# set number of clusters\nkclusters = 5\n\n# run k-means clustering\nkmeans = KMeans(n_clusters = kclusters, random_state=0).fit(el_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# add clustering labels\n\nneighbourhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\nel_merged = east_london\n\n# match/merge EL London data with latitude/longitude for each neighborhood\nel_merged_latlong = el_merged.join(neighbourhoods_venues_sorted.set_index('Neighbourhood'), on = 'Neighbourhood')\nel_merged_latlong.head(5)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Results\n\nVisualise the clusters on the map using folium - coloured keys help to determine what neighbourhoods are similar and fall into what cluster. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# create map\nmap_clusters = folium.Map( location=[latitude, longitude], zoom_start=10)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(el_merged_latlong['Latitude'], el_merged_latlong['Longitude'], el_merged_latlong['Neighbourhood'], el_merged_latlong['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n\nmap_clusters"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Breakdown of each cluster \n\nThis enables us to get a clearer view of how each cluster was formed; determine similarities and trends within the data that lead the model to create these clusters. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cluster 0\ncluster_0 = el_merged_latlong.loc[el_merged_latlong['Cluster Labels'] == 0, el_merged_latlong.columns[[1] + list(range(5, el_merged_latlong.shape[1]))]]\ncluster_0.drop_duplicates()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cluster 1\ncluster_1 = el_merged_latlong.loc[el_merged_latlong['Cluster Labels'] == 1, el_merged_latlong.columns[[1] + list(range(5, el_merged_latlong.shape[1]))]]\ncluster_1.drop_duplicates()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cluster 2\ncluster_2 = el_merged_latlong.loc[el_merged_latlong['Cluster Labels'] == 2, el_merged_latlong.columns[[1] + list(range(5, el_merged_latlong.shape[1]))]]\ncluster_2.drop_duplicates()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cluster 3\ncluster_3 = el_merged_latlong.loc[el_merged_latlong['Cluster Labels'] == 3, el_merged_latlong.columns[[1] + list(range(5, el_merged_latlong.shape[1]))]]\ncluster_3.drop_duplicates()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Cluster 4\ncluster_4 = el_merged_latlong.loc[el_merged_latlong['Cluster Labels'] == 4, el_merged_latlong.columns[[1] + list(range(5, el_merged_latlong.shape[1]))]]\ncluster_4.drop_duplicates()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Discussion\n\nLooking at the individual clusters, the data sets are very similar, however it is the order and frequency that plays a big role in deciding which clusters are better suited to BrightMindsDS criteria. \nWhich were as follows:\n\n- Can it be accessed via local transport?\n- Are there any gyms nearby?\n- Is there a nice park/open space near by?\n- Are there good coffee shops nearby for lunch?\n- Are there good bars/nightclubs to socialise in after work?\n\nBrightMindsDS wanted their chosen location to fulfil at least 2 requirements, exploring each cluster it can be seen that all do this. Which asks the question, what was the point? Well this is where our use of the Foursquare API data played a big role, enabling us to see what venues are the most popular/common in that area.\n\n##### Cluster 0\n- This cluster is definitely one of the largest, made up of nearly 20 neighbourhoods. This offers BrightMindsDS a variety of locations spread out over the East of London. \nThese neighbourhoods have coffee shops and parks listed as the most common venues. Offering many locations to socialise outside of work. This was seen as a great positive for the company as stated in a recent article 'The beauty of a work caf\u00e9, is that it can be treated as both an area for relaxation and a place of engagement and innovation.' (1) This was one of the highest priorities for BrightMindsDS; generating innovation and offering a place to relax and take a break. The 3rd, 4th and 5th most common venues are made up of a variety of bars, gyms and most importantly transport stations. Bus, train, metro and Gas stations are common in these neighbourhoods. Travel in and around London will be essential for a rising company. Also to note, hotel bars and gay bars are listed too; suggesting inclusive cultures and possible venues for clients to stay.\n\n***Based on these factors Neighbourhoods in cluster 2 meet and go beyond BrightMindsDS expectations.*** \n\n##### Cluster 1\n- Within these neighbourhoods, coffee shops and parks were the most popular venues. Suggesting good locations to socialise outside of work. This was seen as a great positive for the company (explained above). However there is very little mention of any local transportation. The realism of working in a big city like London is that employees will need to travel around the city to meet clients and/or commute. This means the neighbourhoods in cluster 1 will have to be ignored.\n\n##### Cluster 2\nLooking at the data for cluster 2 I can see that:\n\n- Park is most common, good space to take a break, promote some outdoor team activities or working\n\n- Train station is also listed as one of the most common venues suggesting good rail and underground links which is great for commuters or when visiting clients.  \n\n- Gym, boxing gym and gym pool mentioned, providing a different experience aside from a weights or cardio gym.\n\n- There are variety of bars on offer too.\n\n***Based on these factors Neighbourhoods in cluster 2 meet and go beyond BrightMindsDS expectations.*** \n\n##### Cluster 3\n- Very similar to neighbourhoods in cluster 1; coffee shops and bars are very common, skate parks and fitness centers offer great facilities for BrightMindsDS employees to stay active. Promoting an active lifestyle and a good work-life balance. However there is no mention of a bus or train station in the vacinity. As mentioned previously, city links and having the option to travel easily is essential to building a steady business. This requirement is a must so these neighbourhoods should be ignored. \n\n##### Cluster 4\n- These 2 neighbourhoods offer good transport links and quite an active location with a gym considered to be the 2nd most common venue. Again, seeing hotel bars suggests that clients can have a place to stay near the office which is a great feature. However there is no data to suggest that a park or open space can be found within the radius of that location. This limits outdoor activities and could inhibit creativity, meaning neighbourhoods in cluster 4 should not be considered. \n\n\n\n(1) https://www.blueprintinteriors.com/office-work-cafe/\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Conclusion"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Looking at these results I would recommend any Neighbourhood classed in cluster 0 or cluster 2.**\n\nBoth fulfil all requirements set out by BrightMindsDS; offering open spaces for employees to relax, recuperate and socialise as a team, have a variety of bars and gyms to escape to if work gets busy. As the company builds and secures more clients, more and more relationships will need to be made, stress may build up and long hours will need to be worked. Providing their employees with a nice office space in an ideal location is essential to improve employee morale and creativity. \nMaking sure they are situated in an up and coming location will reflect well to potential clients and employees alike. \n\nFurther evaluation could have explored office space prices based on location, or reviews of the venues nearby. Foursquare provides so much data it has been very interesting to explore. "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}